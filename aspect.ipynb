{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/amifaraj/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/home/amifaraj/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/home/amifaraj/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/amifaraj/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/home/amifaraj/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/home/amifaraj/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(<Word index=3;text=staff;lemma=staff;upos=NOUN;xpos=NN;feats=Number=Sing;governor=9;dependency_relation=nsubj>,\n",
       "  'det',\n",
       "  <Word index=1;text=The;lemma=the;upos=DET;xpos=DT;feats=Definite=Def|PronType=Art;governor=3;dependency_relation=det>),\n",
       " (<Word index=3;text=staff;lemma=staff;upos=NOUN;xpos=NN;feats=Number=Sing;governor=9;dependency_relation=nsubj>,\n",
       "  'compound',\n",
       "  <Word index=2;text=hotel;lemma=hotel;upos=NOUN;xpos=NN;feats=Number=Sing;governor=3;dependency_relation=compound>),\n",
       " (<Word index=9;text=friendly;lemma=friendly;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>,\n",
       "  'nsubj',\n",
       "  <Word index=3;text=staff;lemma=staff;upos=NOUN;xpos=NN;feats=Number=Sing;governor=9;dependency_relation=nsubj>),\n",
       " (<Word index=5;text=owner;lemma=owner;upos=NOUN;xpos=NN;feats=Number=Sing;governor=3;dependency_relation=conj>,\n",
       "  'cc',\n",
       "  <Word index=4;text=and;lemma=and;upos=CCONJ;xpos=CC;feats=_;governor=5;dependency_relation=cc>),\n",
       " (<Word index=3;text=staff;lemma=staff;upos=NOUN;xpos=NN;feats=Number=Sing;governor=9;dependency_relation=nsubj>,\n",
       "  'conj',\n",
       "  <Word index=5;text=owner;lemma=owner;upos=NOUN;xpos=NN;feats=Number=Sing;governor=3;dependency_relation=conj>),\n",
       " (<Word index=9;text=friendly;lemma=friendly;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>,\n",
       "  'cop',\n",
       "  <Word index=6;text=were;lemma=be;upos=AUX;xpos=VBD;feats=Mood=Ind|Tense=Past|VerbForm=Fin;governor=9;dependency_relation=cop>),\n",
       " (<Word index=9;text=friendly;lemma=friendly;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>,\n",
       "  'advmod',\n",
       "  <Word index=7;text=not;lemma=not;upos=PART;xpos=RB;feats=_;governor=9;dependency_relation=advmod>),\n",
       " (<Word index=9;text=friendly;lemma=friendly;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>,\n",
       "  'advmod',\n",
       "  <Word index=8;text=very;lemma=very;upos=ADV;xpos=RB;feats=_;governor=9;dependency_relation=advmod>),\n",
       " (<Word index=0;text=ROOT>,\n",
       "  'root',\n",
       "  <Word index=9;text=friendly;lemma=friendly;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>),\n",
       " (<Word index=9;text=friendly;lemma=friendly;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>,\n",
       "  'punct',\n",
       "  <Word index=10;text=.;lemma=.;upos=PUNCT;xpos=.;feats=_;governor=9;dependency_relation=punct>)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "stanfordnlp.download('en')   # This downloads the English models for the neural pipeline\n",
    "# IMPORTANT: The above line prompts you before downloading, which doesn't work well in a Jupyter notebook.\n",
    "# To avoid a prompt when using notebooks, instead use: >>> stanfordnlp.download('en', force=True)\n",
    "nlp = stanfordnlp.Pipeline()\n",
    "#nlp = stanfordnlp.Pipeline() # This sets up a default neural pipeline in English\n",
    "\n",
    "doc.sentences[0].dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'posTag'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-150cfe647d17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Barack Obama was born in Hawaii.  He was a good person.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposTag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'posTag'"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"Barack Obama was born in Hawaii.  He was a good person.\"\n",
    "text.posTag(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sentence' object has no attribute 'parseTree'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-c78c7037d02e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstanfordnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCoreNLPClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdependency_parse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sentence' object has no attribute 'parseTree'"
     ]
    }
   ],
   "source": [
    "dependency_parse = doc.sentences[0].parseTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'StanfordCoreNLP' from 'stanfordnlp' (/home/amifaraj/anaconda3/lib/python3.7/site-packages/stanfordnlp/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-a5f82986b488>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#from nltk.parse.stanford import StanfordDependencyParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstanfordnlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstanfordnlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStanfordCoreNLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpath_to_jar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'stanford-corenlp-full-2018-10-05.zip'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpath_to_models_jar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'stanford-english-kbp-corenlp-2018-10-05-models.jar'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'StanfordCoreNLP' from 'stanfordnlp' (/home/amifaraj/anaconda3/lib/python3.7/site-packages/stanfordnlp/__init__.py)"
     ]
    }
   ],
   "source": [
    "#from nltk.parse.stanford import StanfordDependencyParser\n",
    "import stanfordnlp\n",
    "from stanfordnlp import StanfordCoreNLP\n",
    "path_to_jar = 'stanford-corenlp-full-2018-10-05.zip'\n",
    "path_to_models_jar = 'stanford-english-kbp-corenlp-2018-10-05-models.jar'\n",
    "\n",
    "dependency_parser = StanfordCoreNLP(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "result = dependency_parser.raw_parse('I shot an elephant in my sleep')\n",
    "dep = result.next()\n",
    "\n",
    "list(dep.triples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/amifaraj/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('He', '5', 'nsubj')\n",
      "('was', '5', 'cop')\n",
      "('a', '5', 'det')\n",
      "('good', '5', 'amod')\n",
      "('person', '0', 'root')\n",
      "('.', '5', 'punct')\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Barack Obama was born in Hawaii.  He was a good person.\")\n",
    "d = doc.sentences[1].print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Token: [('Barack', 'NNP'), ('Obama', 'NNP'), ('was', 'VBD'), ('born', 'VBN'), ('in', 'IN'), ('Hawaii.', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "text =\"Barack Obama was born in Hawaii.\".split()\n",
    "#print(\"After Split:\",text)\n",
    "tokens_tag = pos_tag(text)\n",
    "print(\"After Token:\",tokens_tag)\n",
    "#patterns= \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
    "#chunker = RegexpParser(patterns)\n",
    "#print(\"After Regex:\",chunker)\n",
    "#output = chunker.parse(tokens_tag)\n",
    "#print(\"After Chunking\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hotel</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>staff,</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>manager</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>owner</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>were</td>\n",
       "      <td>VBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>not</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>very</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>friendly</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>supportive.</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word POS tag\n",
       "0           The      DT\n",
       "1         hotel      NN\n",
       "2        staff,      NN\n",
       "3       manager      NN\n",
       "4           and      CC\n",
       "5         owner      NN\n",
       "6          were     VBD\n",
       "7           not      RB\n",
       "8          very      RB\n",
       "9      friendly      JJ\n",
       "10          and      CC\n",
       "11  supportive.      NN"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "text = \"The hotel staff, manager and owner were not very friendly and supportive.\"\n",
    "nltk_pos_tagged = pos_tag(text.split())\n",
    "pd.DataFrame(nltk_pos_tagged, columns=['Word', 'POS tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/amifaraj/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/home/amifaraj/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/home/amifaraj/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/amifaraj/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/home/amifaraj/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/home/amifaraj/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gov</th>\n",
       "      <th>rel</th>\n",
       "      <th>dep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;Word index=5;text=scientist;lemma=scientist;u...</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>&lt;Word index=1;text=Bill;lemma=Bill;upos=PROPN;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;Word index=5;text=scientist;lemma=scientist;u...</td>\n",
       "      <td>cop</td>\n",
       "      <td>&lt;Word index=2;text=is;lemma=be;upos=AUX;xpos=V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;Word index=5;text=scientist;lemma=scientist;u...</td>\n",
       "      <td>advmod</td>\n",
       "      <td>&lt;Word index=3;text=not;lemma=not;upos=PART;xpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;Word index=5;text=scientist;lemma=scientist;u...</td>\n",
       "      <td>det</td>\n",
       "      <td>&lt;Word index=4;text=a;lemma=a;upos=DET;xpos=DT;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;Word index=0;text=ROOT&gt;</td>\n",
       "      <td>root</td>\n",
       "      <td>&lt;Word index=5;text=scientist;lemma=scientist;u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 gov     rel  \\\n",
       "0  <Word index=5;text=scientist;lemma=scientist;u...   nsubj   \n",
       "1  <Word index=5;text=scientist;lemma=scientist;u...     cop   \n",
       "2  <Word index=5;text=scientist;lemma=scientist;u...  advmod   \n",
       "3  <Word index=5;text=scientist;lemma=scientist;u...     det   \n",
       "4                           <Word index=0;text=ROOT>    root   \n",
       "\n",
       "                                                 dep  \n",
       "0  <Word index=1;text=Bill;lemma=Bill;upos=PROPN;...  \n",
       "1  <Word index=2;text=is;lemma=be;upos=AUX;xpos=V...  \n",
       "2  <Word index=3;text=not;lemma=not;upos=PART;xpo...  \n",
       "3  <Word index=4;text=a;lemma=a;upos=DET;xpos=DT;...  \n",
       "4  <Word index=5;text=scientist;lemma=scientist;u...  "
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "nlp = stanfordnlp.Pipeline()\n",
    "#nlp = stanfordnlp.Pipeline() # This sets up a default neural pipeline in English\n",
    "\n",
    "#doc.sentences.dependencies\n",
    "text = 'Bill is not a scientist'\n",
    "doc = nlp(text)\n",
    "df = pd.DataFrame(doc.sentences[0].dependencies, columns=['gov', 'rel', 'dep'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['staff', 'manager', 'owner']"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nounList=['staff']\n",
    "nounList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(0,len(df)):\n",
    "    if df['rel'][d]=='conj' and df['gov'][d].lemma=='staff':\n",
    "        nounList.append(df['dep'][d].lemma)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<Word index=3;text=staff;lemma=staff;upos=NOUN;xpos=NN;feats=Number=Sing;governor=11;dependency_relation=nsubj>,\n",
       "  'det',\n",
       "  <Word index=1;text=The;lemma=the;upos=DET;xpos=DT;feats=Definite=Def|PronType=Art;governor=3;dependency_relation=det>),\n",
       " (<Word index=3;text=staff;lemma=staff;upos=NOUN;xpos=NN;feats=Number=Sing;governor=11;dependency_relation=nsubj>,\n",
       "  'compound',\n",
       "  <Word index=2;text=hotel;lemma=hotel;upos=NOUN;xpos=NN;feats=Number=Sing;governor=3;dependency_relation=compound>),\n",
       " (<Word index=11;text=friendly;lemma=friendly;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>,\n",
       "  'nsubj',\n",
       "  <Word index=3;text=staff;lemma=staff;upos=NOUN;xpos=NN;feats=Number=Sing;governor=11;dependency_relation=nsubj>),\n",
       " (<Word index=5;text=manager;lemma=manager;upos=NOUN;xpos=NN;feats=Number=Sing;governor=3;dependency_relation=conj>,\n",
       "  'punct',\n",
       "  <Word index=4;text=,;lemma=,;upos=PUNCT;xpos=,;feats=_;governor=5;dependency_relation=punct>),\n",
       " (<Word index=3;text=staff;lemma=staff;upos=NOUN;xpos=NN;feats=Number=Sing;governor=11;dependency_relation=nsubj>,\n",
       "  'conj',\n",
       "  <Word index=5;text=manager;lemma=manager;upos=NOUN;xpos=NN;feats=Number=Sing;governor=3;dependency_relation=conj>),\n",
       " (<Word index=7;text=owner;lemma=owner;upos=NOUN;xpos=NN;feats=Number=Sing;governor=3;dependency_relation=conj>,\n",
       "  'cc',\n",
       "  <Word index=6;text=and;lemma=and;upos=CCONJ;xpos=CC;feats=_;governor=7;dependency_relation=cc>),\n",
       " (<Word index=3;text=staff;lemma=staff;upos=NOUN;xpos=NN;feats=Number=Sing;governor=11;dependency_relation=nsubj>,\n",
       "  'conj',\n",
       "  <Word index=7;text=owner;lemma=owner;upos=NOUN;xpos=NN;feats=Number=Sing;governor=3;dependency_relation=conj>),\n",
       " (<Word index=11;text=friendly;lemma=friendly;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>,\n",
       "  'cop',\n",
       "  <Word index=8;text=were;lemma=be;upos=AUX;xpos=VBD;feats=Mood=Ind|Tense=Past|VerbForm=Fin;governor=11;dependency_relation=cop>),\n",
       " (<Word index=11;text=friendly;lemma=friendly;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>,\n",
       "  'advmod',\n",
       "  <Word index=9;text=not;lemma=not;upos=PART;xpos=RB;feats=_;governor=11;dependency_relation=advmod>),\n",
       " (<Word index=11;text=friendly;lemma=friendly;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>,\n",
       "  'advmod',\n",
       "  <Word index=10;text=very;lemma=very;upos=ADV;xpos=RB;feats=_;governor=11;dependency_relation=advmod>),\n",
       " (<Word index=0;text=ROOT>,\n",
       "  'root',\n",
       "  <Word index=11;text=friendly;lemma=friendly;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>),\n",
       " (<Word index=13;text=supportive;lemma=supportive;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=11;dependency_relation=conj>,\n",
       "  'cc',\n",
       "  <Word index=12;text=and;lemma=and;upos=CCONJ;xpos=CC;feats=_;governor=13;dependency_relation=cc>),\n",
       " (<Word index=11;text=friendly;lemma=friendly;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>,\n",
       "  'conj',\n",
       "  <Word index=13;text=supportive;lemma=supportive;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=11;dependency_relation=conj>),\n",
       " (<Word index=11;text=friendly;lemma=friendly;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>,\n",
       "  'punct',\n",
       "  <Word index=14;text=.;lemma=.;upos=PUNCT;xpos=.;feats=_;governor=11;dependency_relation=punct>)]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sentences[0].dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "det\n",
      "compound\n",
      "nsubj\n",
      "punct\n",
      "conj\n",
      "cc\n",
      "conj\n",
      "cop\n",
      "advmod\n",
      "advmod\n",
      "root\n",
      "cc\n",
      "conj\n",
      "punct\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(doc.sentences[0].dependencies)):\n",
    "    print(doc.sentences[0].dependencies[i][2].dependency_relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc.sentences[0].dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gov: NOUN\n",
      "det\n",
      "dep: DET\n",
      "****\n",
      "gov: NOUN\n",
      "compound\n",
      "dep: NOUN\n",
      "****\n",
      "gov: ADJ\n",
      "nsubj\n",
      "dep: NOUN\n",
      "****\n",
      "gov: NOUN\n",
      "punct\n",
      "dep: PUNCT\n",
      "****\n",
      "gov: NOUN\n",
      "conj\n",
      "dep: NOUN\n",
      "****\n",
      "gov: NOUN\n",
      "cc\n",
      "dep: CCONJ\n",
      "****\n",
      "gov: NOUN\n",
      "conj\n",
      "dep: NOUN\n",
      "****\n",
      "gov: ADJ\n",
      "cop\n",
      "dep: AUX\n",
      "****\n",
      "gov: ADJ\n",
      "advmod\n",
      "dep: PART\n",
      "****\n",
      "gov: ADJ\n",
      "advmod\n",
      "dep: ADV\n",
      "****\n",
      "gov: None\n",
      "root\n",
      "dep: ADJ\n",
      "****\n",
      "gov: ADJ\n",
      "cc\n",
      "dep: CCONJ\n",
      "****\n",
      "gov: ADJ\n",
      "conj\n",
      "dep: ADJ\n",
      "****\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(doc.sentences[0].dependencies)-1):\n",
    "    print(\"gov: \"+ str(doc.sentences[0].dependencies[i][0].upos))\n",
    "    print(doc.sentences[0].dependencies[i][1]) \n",
    "    print(\"dep: \"+ doc.sentences[0].dependencies[i][2].upos)\n",
    "    print(\"****\")               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hotel\n",
      "det\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "print(doc.sentences[0].dependencies[0][0].lemma)\n",
    "print(doc.sentences[0].dependencies[0][1])\n",
    "print(doc.sentences[0].dependencies[0][2].lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<nsubj.?>*<VBD.?>*<JJ.?>*<CC>?'>\n"
     ]
    }
   ],
   "source": [
    "patterns= \"\"\"mychunk:{<nsubj.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
    "chunker = RegexpParser(patterns)\n",
    "print(\"After Regex:\",chunker)\n",
    "#output = chunker.parse(tokens_tag)\n",
    "#print(\"After Chunking\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"The hotel staff and owner were not very friendly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amifaraj/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.StanforCoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('friendly', [Tree('staff', ['The', 'hotel', 'and', 'owner']), 'were', 'not', 'very'])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "sdp = StanfordDependencyParser(path_to_jar='stanford/stanford-corenlp-3.9.2.jar',path_to_models_jar='stanford/stanford-corenlp-3.9.2-models.jar')    \n",
    "\n",
    "result = list(sdp.raw_parse(text))  \n",
    "\n",
    "# print the dependency tree\n",
    "dep_tree = [parse.tree() for parse in result][0]\n",
    "dep_tree\n",
    "\n",
    "# visualize raw dependency tree\n",
    "#from IPython.display import display\n",
    "#display(dep_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. extractDependencies(Sentence)\n",
    "2. getNounSiblings(d.dep)\n",
    "3. getAdjectiveSiblings(d.gov)\n",
    "4. isAffirmative(?)\n",
    "5. isAdjective\n",
    "6. isVerb\n",
    "7. isNoun()\n",
    "8. isPronoun()\n",
    "9. getAdjectiveModifiers()\n",
    "10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractDependencies(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    dependency = doc.sentences[0].dependencies\n",
    "    depTable = pd.DataFrame(dependency, columns=['gov', 'rel', 'dep'])\n",
    "    return depTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNounSibling(depTable, nountoken, adjtoken):\n",
    "    ## I add this ##\n",
    "    if isPronoun(nountoken):\n",
    "        for index in range(0,len(depTable)):\n",
    "            if depTable['gov'][index].lemma == adjtoken.lemma and isNoun(depTable['dep'][index]):\n",
    "                noun = depTable['dep'][index].lemma\n",
    "                nounList=[noun]\n",
    "    elif isNoun(nountoken):\n",
    "        noun = nountoken.lemma\n",
    "        nounList=[noun]\n",
    "    for index in range(0,len(depTable)):\n",
    "        if depTable['rel'][index] =='conj' and depTable['gov'][index].lemma == noun:\n",
    "            nounList.append(depTable['dep'][index].lemma)\n",
    "    return nounList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isdependent(depTable, newadj):\n",
    "    for ind in range(0,len(depTable)):\n",
    "        ## if the newadj is not a gov of nsub which is not dependant on adj\n",
    "        if 'nsubj' in depTable['rel'][ind] and depTable['gov'][ind].lemma == newadj:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAdjectiveSibling(depTable, adj, nouns):\n",
    "    adjective = adj.lemma\n",
    "    adjList=[adjective]\n",
    "    for index in range(0,len(depTable)):\n",
    "        if depTable['rel'][index] =='conj' and depTable['gov'][index].lemma == adjective:\n",
    "            newadj = depTable['dep'][index].lemma\n",
    "            if isdependent(depTable, newadj):  ## if both adj describing same aspect\n",
    "                adjList.append(newadj)\n",
    "    return adjList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAdjectiveModifiers(depTable, adj):\n",
    "    #adjective = adj.lemma    \n",
    "    adjmod='_'\n",
    "    for index in range(0,len(depTable)):\n",
    "        if depTable['rel'][index] =='advmod' and depTable['gov'][index].lemma == adj:\n",
    "            adjmod = depTable['dep'][index].lemma\n",
    "    return adjmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNoun(token):\n",
    "    POS = token.xpos\n",
    "    if 'NN' in POS:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isPronoun(token):\n",
    "    POS = token.xpos\n",
    "    if 'PR' in POS:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isAdjective(token):\n",
    "    POS = token.xpos\n",
    "    if 'JJ' in POS:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isVerb(token):\n",
    "    POS = token.xpos\n",
    "    if 'VB' in POS:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isAdverb(token):\n",
    "    POS = token.xpos\n",
    "    if 'RB' in POS:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeNeg(depTable, Col):\n",
    "    for index in range(0,len(depTable)):\n",
    "        if depTable['rel'][index] =='advmod' and 'Neg' in depTable[Col][index].feats:\n",
    "            depTable['rel'][index] ='neg'\n",
    "    return depTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isAffirmative(depTable, adj):\n",
    "    #adjective = adj.lemma\n",
    "    for index in range(0,len(depTable)):\n",
    "        if depTable['rel'][index] =='neg' and depTable['gov'][index].lemma == adj:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotations(nouns, adj, mod, aff):\n",
    "    #a =[]\n",
    "    for n in nouns:\n",
    "        t = tuple((n, adj, mod, aff))\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casensub(df, index):\n",
    "    dep = df['dep'][index]\n",
    "    gov = df['gov'][index]           # getting the dependence of the noun (looking for adj or vb)\n",
    "    nouns = getNounSibling(df, dep, gov)  #getting the nouns that are joined by conjuction\n",
    "    a =[]\n",
    "    if isAdjective(gov):            ## example: the room is big\n",
    "        adjs = getAdjectiveSibling(df, gov, nouns)  #getting the adjective that are joined by conj\n",
    "        for adj in adjs:                     #for all the adj check the modifier and affirmative\n",
    "            mod = getAdjectiveModifiers(df, adj)\n",
    "            aff = isAffirmative(df, adj)\n",
    "            a.append(annotations(nouns, adj, mod, aff))\n",
    "        return a\n",
    "    elif isVerb(gov):         #example:The staff works fast\n",
    "        for ind in range(0, len(df)):\n",
    "            if (df['rel'][ind] == 'xcomp' or df['rel'][ind] == 'advmod') and df['gov'][ind].lemma == gov.lemma:    ##looking for the adjective or adverb\n",
    "                newdep = df['dep'][ind]                ## adj or adv\n",
    "                adjs = getAdjectiveSibling(df, newdep, nouns)   ## list of adj joined by conj\n",
    "                for adj in adjs:                     #for all the adj check the modifier and affirmative\n",
    "                    mod = getAdjectiveModifiers(df, adj)\n",
    "                    aff = isAffirmative(df, adj)\n",
    "                    a.append(annotations(nouns, adj, mod, aff))\n",
    "                return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caseamod(df, index):   # example: The restaurant has good staff\n",
    "    dep = df['dep'][index]\n",
    "    gov = df['gov'][index]\n",
    "    a =[]\n",
    "    if isAdjective(dep) or isAdverb(dep) and (isNoun(gov) or isPronoun(gov)):\n",
    "        nouns = getNounSibling(df, gov, dep)  #gov is a noun and dep is an adj\n",
    "        adjs = getAdjectiveSibling(df, dep, nouns)  # dep is the adj\n",
    "        for adj in adjs:                     #for all the adj check the modifier and affirmative\n",
    "            mod = getAdjectiveModifiers(df, adj)\n",
    "            aff = isAffirmative(df, adj)\n",
    "            a.append(annotations(nouns, adj, mod, aff))\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casecomp1(df, index):\n",
    "    dep = df['dep'][index]  #adj\n",
    "    gov = df['gov'][index]  #verb\n",
    "    a =[]\n",
    "    if (isNoun(dep) or isPronoun(dep) or isVerb(dep)) or isAdjective(dep):\n",
    "        if isVerb(dep):\n",
    "            noun = getsubj(df, dep)\n",
    "        nouns = getNounSibling(df, dep, gov)\n",
    "        adjs = getAdjectiveSibling(df, gov, nouns)\n",
    "        for adj in adjs:                     #for all the adj check the modifier and affirmative\n",
    "            mod = getAdjectiveModifiers(df, adj)\n",
    "            aff = isAffirmative(df, adj)\n",
    "            a.append(annotations(nouns, adj, mod, aff))\n",
    "        #mods = getAdjectiveModifiers(df, gov)\n",
    "        #aff = isAffirmative(df, dep)\n",
    "        #a = annotations(nouns, adjs, mods, aff)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsubj(depTable, verb):\n",
    "    for ind in range(0,len(depTable)):\n",
    "        if df['rel'][ind] == 'nsubj' and df['gov'][ind].lemma == verb.lemma:\n",
    "            #print(df['dep'][index])\n",
    "            return df['dep'][ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casecomp(df, index):\n",
    "    dep = df['dep'][index]  #adj\n",
    "    gov = df['gov'][index]  #verb\n",
    "    a =[]\n",
    "    if isAdjective(dep):\n",
    "        sub = getsubj(df, gov)\n",
    "        nouns = getNounSibling(df, sub, dep)  #(df, n, adj)\n",
    "        adjs = getAdjectiveSibling(df, dep, nouns)\n",
    "        for adj in adjs:                     #for all the adj check the modifier and affirmative\n",
    "            mod = getAdjectiveModifiers(df, adj)\n",
    "            aff = isAffirmative(df, adj)\n",
    "            a.append(annotations(nouns, adj, mod, aff))\n",
    "        #mods = getAdjectiveModifiers(df, gov)\n",
    "        #aff = isAffirmative(df, dep)\n",
    "        #a = annotations(nouns, adjs, mods, aff)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/amifaraj/stanfordnlp_resources/en_gum_models/en_gum_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_gum', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/home/amifaraj/stanfordnlp_resources/en_gum_models/en_gum_tagger.pt', 'pretrain_path': '/home/amifaraj/stanfordnlp_resources/en_gum_models/en_gum.pretrain.pt', 'lang': 'en', 'shorthand': 'en_gum', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/amifaraj/stanfordnlp_resources/en_gum_models/en_gum_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_gum', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/home/amifaraj/stanfordnlp_resources/en_gum_models/en_gum_parser.pt', 'pretrain_path': '/home/amifaraj/stanfordnlp_resources/en_gum_models/en_gum.pretrain.pt', 'lang': 'en', 'shorthand': 'en_gum', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "#stanfordnlp.download('en', version='0.2.0' )   # This downloads the English models for the neural pipeline\n",
    "# IMPORTANT: The above line prompts you before downloading, which doesn't work well in a Jupyter notebook.\n",
    "# To avoid a prompt when using notebooks, instead use: >>> stanfordnlp.download('en', force=True)\n",
    "nlp = stanfordnlp.Pipeline(lang='en', treebank ='en_gum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gov</th>\n",
       "      <th>rel</th>\n",
       "      <th>dep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;Word index=2;text=game;lemma=game;upos=NOUN;x...</td>\n",
       "      <td>det</td>\n",
       "      <td>&lt;Word index=1;text=the;lemma=the;upos=DET;xpos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;Word index=5;text=cool;lemma=cool;upos=ADJ;xp...</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>&lt;Word index=2;text=game;lemma=game;upos=NOUN;x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;Word index=5;text=cool;lemma=cool;upos=ADJ;xp...</td>\n",
       "      <td>cop</td>\n",
       "      <td>&lt;Word index=3;text=is;lemma=be;upos=AUX;xpos=V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;Word index=5;text=cool;lemma=cool;upos=ADJ;xp...</td>\n",
       "      <td>neg</td>\n",
       "      <td>&lt;Word index=4;text=not;lemma=not;upos=PART;xpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;Word index=0;text=ROOT&gt;</td>\n",
       "      <td>root</td>\n",
       "      <td>&lt;Word index=5;text=cool;lemma=cool;upos=ADJ;xp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;Word index=5;text=cool;lemma=cool;upos=ADJ;xp...</td>\n",
       "      <td>punct</td>\n",
       "      <td>&lt;Word index=6;text=.;lemma=.;upos=PUNCT;xpos=....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 gov    rel  \\\n",
       "0  <Word index=2;text=game;lemma=game;upos=NOUN;x...    det   \n",
       "1  <Word index=5;text=cool;lemma=cool;upos=ADJ;xp...  nsubj   \n",
       "2  <Word index=5;text=cool;lemma=cool;upos=ADJ;xp...    cop   \n",
       "3  <Word index=5;text=cool;lemma=cool;upos=ADJ;xp...    neg   \n",
       "4                           <Word index=0;text=ROOT>   root   \n",
       "5  <Word index=5;text=cool;lemma=cool;upos=ADJ;xp...  punct   \n",
       "\n",
       "                                                 dep  \n",
       "0  <Word index=1;text=the;lemma=the;upos=DET;xpos...  \n",
       "1  <Word index=2;text=game;lemma=game;upos=NOUN;x...  \n",
       "2  <Word index=3;text=is;lemma=be;upos=AUX;xpos=V...  \n",
       "3  <Word index=4;text=not;lemma=not;upos=PART;xpo...  \n",
       "4  <Word index=5;text=cool;lemma=cool;upos=ADJ;xp...  \n",
       "5  <Word index=6;text=.;lemma=.;upos=PUNCT;xpos=....  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "text = \"the game is  not cool.\"\n",
    "#text = \"The staff was not good\"\n",
    "df = extractDependencies(text)\n",
    "df = makeNeg(df, 'dep')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('game', 'cool', '_', False)]]\n"
     ]
    }
   ],
   "source": [
    "A = []\n",
    "for index in range(0, len(df)):\n",
    "    if 'nsubj' in df['rel'][index] and isNoun(df['dep'][index]):\n",
    "        annot = casensub(df, index)\n",
    "        if annot is not None and annot not in A:\n",
    "            A.append(annot)\n",
    "    elif 'amod' in df['rel'][index]:\n",
    "        annot = caseamod(df, index)\n",
    "        if annot is not None and annot not in A:\n",
    "            A.append(annot)\n",
    "    elif 'comp' in df['rel'][index] and isVerb(df['gov'][index]):\n",
    "        annot = casecomp(df, index)\n",
    "        if annot is not None and annot not in A:\n",
    "            A.append(annot)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: What \tupos: PRON\txpos: WP\n",
      "word: would \tupos: AUX\txpos: MD\n",
      "word: happen \tupos: VERB\txpos: VB\n",
      "word: if \tupos: SCONJ\txpos: IN\n",
      "word: the \tupos: DET\txpos: DT\n",
      "word: Indian \tupos: ADJ\txpos: JJ\n",
      "word: government \tupos: NOUN\txpos: NN\n",
      "word: stole \tupos: VERB\txpos: VBD\n",
      "word: the \tupos: DET\txpos: DT\n",
      "word: Kohinoor \tupos: PROPN\txpos: NNP\n",
      "word: ( \tupos: PUNCT\txpos: -LRB-\n",
      "word: Koh \tupos: PROPN\txpos: NNP\n",
      "word: - \tupos: PUNCT\txpos: HYPH\n",
      "word: i \tupos: PROPN\txpos: NNP\n",
      "word: - \tupos: PUNCT\txpos: HYPH\n",
      "word: Noor \tupos: PROPN\txpos: NNP\n",
      "word: ) \tupos: PUNCT\txpos: -RRB-\n",
      "word: diamond \tupos: NOUN\txpos: NN\n",
      "word: back \tupos: ADV\txpos: RB\n",
      "word: ? \tupos: PUNCT\txpos: .\n"
     ]
    }
   ],
   "source": [
    "#import stanfordnlp\n",
    "\n",
    "#nlp = stanfordnlp.Pipeline(processors='tokenize,mwt,pos')\n",
    "doc1 = nlp(\"What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?\")\n",
    "doc2 = nlp(\"What is the story of Kohinoor (Koh-i-Noor) Diamond?\")\n",
    "print(*[f'word: {word.text+\" \"}\\tupos: {word.upos}\\txpos: {word.xpos}' for sent in doc1.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: What \tupos: PRON\txpos: WP\n",
      "word: is \tupos: AUX\txpos: VBZ\n",
      "word: the \tupos: DET\txpos: DT\n",
      "word: story \tupos: NOUN\txpos: NN\n",
      "word: of \tupos: ADP\txpos: IN\n",
      "word: Kohinoor \tupos: PROPN\txpos: NNP\n",
      "word: ( \tupos: PUNCT\txpos: -LRB-\n",
      "word: Koh \tupos: PROPN\txpos: NNP\n",
      "word: - \tupos: PUNCT\txpos: HYPH\n",
      "word: i \tupos: PROPN\txpos: NNP\n",
      "word: - \tupos: PUNCT\txpos: HYPH\n",
      "word: Noor \tupos: PROPN\txpos: NNP\n",
      "word: ) \tupos: PUNCT\txpos: -RRB-\n",
      "word: Diamond \tupos: PROPN\txpos: NNP\n",
      "word: ? \tupos: PUNCT\txpos: .\n"
     ]
    }
   ],
   "source": [
    "print(*[f'word: {word.text+\" \"}\\tupos: {word.upos}\\txpos: {word.xpos}' for sent in doc2.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms: ['thinking', 'thought', 'thought_process', 'cerebration', 'intellection', 'mentation', 'think', 'believe', 'consider', 'conceive', 'think', 'opine', 'suppose', 'imagine', 'reckon', 'guess', 'think', 'cogitate', 'cerebrate', 'remember', 'retrieve', 'recall', 'call_back', 'call_up', 'recollect', 'think', 'think', 'think', 'intend', 'mean', 'think', 'think', 'think', 'think', 'think', 'think', 'think', 'intelligent', 'reasoning', 'thinking']\n",
      "Antonyms: ['forget']\n"
     ]
    }
   ],
   "source": [
    "   #Import wordnet from the NLTK\n",
    "syn = list()\n",
    "ant = list()\n",
    "for synset in wordnet.synsets(\"thinking\"):\n",
    "    for lemma in synset.lemmas():\n",
    "        syn.append(lemma.name())    #add the synonyms\n",
    "        if lemma.antonyms():    #When antonyms are available, add them into the list\n",
    "            ant.append(lemma.antonyms()[0].name())\n",
    "print('Synonyms: ' + str(syn))\n",
    "print('Antonyms: ' + str(ant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
