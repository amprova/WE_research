{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/amifaraj/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/home/amifaraj/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/home/amifaraj/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/amifaraj/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/home/amifaraj/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/home/amifaraj/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(<Word index=3;text=staff;lemma=staff;upos=NOUN;xpos=NN;feats=Number=Sing;governor=9;dependency_relation=nsubj>,\n",
       "  'det',\n",
       "  <Word index=1;text=The;lemma=the;upos=DET;xpos=DT;feats=Definite=Def|PronType=Art;governor=3;dependency_relation=det>),\n",
       " (<Word index=3;text=staff;lemma=staff;upos=NOUN;xpos=NN;feats=Number=Sing;governor=9;dependency_relation=nsubj>,\n",
       "  'compound',\n",
       "  <Word index=2;text=hotel;lemma=hotel;upos=NOUN;xpos=NN;feats=Number=Sing;governor=3;dependency_relation=compound>),\n",
       " (<Word index=9;text=friendly;lemma=friendly;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>,\n",
       "  'nsubj',\n",
       "  <Word index=3;text=staff;lemma=staff;upos=NOUN;xpos=NN;feats=Number=Sing;governor=9;dependency_relation=nsubj>),\n",
       " (<Word index=5;text=owner;lemma=owner;upos=NOUN;xpos=NN;feats=Number=Sing;governor=3;dependency_relation=conj>,\n",
       "  'cc',\n",
       "  <Word index=4;text=and;lemma=and;upos=CCONJ;xpos=CC;feats=_;governor=5;dependency_relation=cc>),\n",
       " (<Word index=3;text=staff;lemma=staff;upos=NOUN;xpos=NN;feats=Number=Sing;governor=9;dependency_relation=nsubj>,\n",
       "  'conj',\n",
       "  <Word index=5;text=owner;lemma=owner;upos=NOUN;xpos=NN;feats=Number=Sing;governor=3;dependency_relation=conj>),\n",
       " (<Word index=9;text=friendly;lemma=friendly;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>,\n",
       "  'cop',\n",
       "  <Word index=6;text=were;lemma=be;upos=AUX;xpos=VBD;feats=Mood=Ind|Tense=Past|VerbForm=Fin;governor=9;dependency_relation=cop>),\n",
       " (<Word index=9;text=friendly;lemma=friendly;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>,\n",
       "  'advmod',\n",
       "  <Word index=7;text=not;lemma=not;upos=PART;xpos=RB;feats=_;governor=9;dependency_relation=advmod>),\n",
       " (<Word index=9;text=friendly;lemma=friendly;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>,\n",
       "  'advmod',\n",
       "  <Word index=8;text=very;lemma=very;upos=ADV;xpos=RB;feats=_;governor=9;dependency_relation=advmod>),\n",
       " (<Word index=0;text=ROOT>,\n",
       "  'root',\n",
       "  <Word index=9;text=friendly;lemma=friendly;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>),\n",
       " (<Word index=9;text=friendly;lemma=friendly;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>,\n",
       "  'punct',\n",
       "  <Word index=10;text=.;lemma=.;upos=PUNCT;xpos=.;feats=_;governor=9;dependency_relation=punct>)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "stanfordnlp.download('en')   # This downloads the English models for the neural pipeline\n",
    "# IMPORTANT: The above line prompts you before downloading, which doesn't work well in a Jupyter notebook.\n",
    "# To avoid a prompt when using notebooks, instead use: >>> stanfordnlp.download('en', force=True)\n",
    "nlp = stanfordnlp.Pipeline()\n",
    "#nlp = stanfordnlp.Pipeline() # This sets up a default neural pipeline in English\n",
    "\n",
    "doc.sentences[0].dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'posTag'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-150cfe647d17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Barack Obama was born in Hawaii.  He was a good person.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposTag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'posTag'"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"Barack Obama was born in Hawaii.  He was a good person.\"\n",
    "text.posTag(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sentence' object has no attribute 'parseTree'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-c78c7037d02e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstanfordnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCoreNLPClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdependency_parse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sentence' object has no attribute 'parseTree'"
     ]
    }
   ],
   "source": [
    "dependency_parse = doc.sentences[0].parseTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'StanfordCoreNLP' from 'stanfordnlp' (/home/amifaraj/anaconda3/lib/python3.7/site-packages/stanfordnlp/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-a5f82986b488>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#from nltk.parse.stanford import StanfordDependencyParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstanfordnlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstanfordnlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStanfordCoreNLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpath_to_jar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'stanford-corenlp-full-2018-10-05.zip'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpath_to_models_jar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'stanford-english-kbp-corenlp-2018-10-05-models.jar'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'StanfordCoreNLP' from 'stanfordnlp' (/home/amifaraj/anaconda3/lib/python3.7/site-packages/stanfordnlp/__init__.py)"
     ]
    }
   ],
   "source": [
    "#from nltk.parse.stanford import StanfordDependencyParser\n",
    "import stanfordnlp\n",
    "from stanfordnlp import StanfordCoreNLP\n",
    "path_to_jar = 'stanford-corenlp-full-2018-10-05.zip'\n",
    "path_to_models_jar = 'stanford-english-kbp-corenlp-2018-10-05-models.jar'\n",
    "\n",
    "dependency_parser = StanfordCoreNLP(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "result = dependency_parser.raw_parse('I shot an elephant in my sleep')\n",
    "dep = result.next()\n",
    "\n",
    "list(dep.triples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/amifaraj/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('He', '5', 'nsubj')\n",
      "('was', '5', 'cop')\n",
      "('a', '5', 'det')\n",
      "('good', '5', 'amod')\n",
      "('person', '0', 'root')\n",
      "('.', '5', 'punct')\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Barack Obama was born in Hawaii.  He was a good person.\")\n",
    "d = doc.sentences[1].print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Token: [('Barack', 'NNP'), ('Obama', 'NNP'), ('was', 'VBD'), ('born', 'VBN'), ('in', 'IN'), ('Hawaii.', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "text =\"Barack Obama was born in Hawaii.\".split()\n",
    "#print(\"After Split:\",text)\n",
    "tokens_tag = pos_tag(text)\n",
    "print(\"After Token:\",tokens_tag)\n",
    "#patterns= \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
    "#chunker = RegexpParser(patterns)\n",
    "#print(\"After Regex:\",chunker)\n",
    "#output = chunker.parse(tokens_tag)\n",
    "#print(\"After Chunking\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hotel</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>staff,</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>manager</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>owner</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>were</td>\n",
       "      <td>VBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>not</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>very</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>friendly</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>supportive.</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word POS tag\n",
       "0           The      DT\n",
       "1         hotel      NN\n",
       "2        staff,      NN\n",
       "3       manager      NN\n",
       "4           and      CC\n",
       "5         owner      NN\n",
       "6          were     VBD\n",
       "7           not      RB\n",
       "8          very      RB\n",
       "9      friendly      JJ\n",
       "10          and      CC\n",
       "11  supportive.      NN"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "text = \"The hotel staff, manager and owner were not very friendly and supportive.\"\n",
    "nltk_pos_tagged = pos_tag(text.split())\n",
    "pd.DataFrame(nltk_pos_tagged, columns=['Word', 'POS tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/amifaraj/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/home/amifaraj/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/home/amifaraj/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/amifaraj/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/home/amifaraj/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/home/amifaraj/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gov</th>\n",
       "      <th>rel</th>\n",
       "      <th>dep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;Word index=5;text=scientist;lemma=scientist;u...</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>&lt;Word index=1;text=Bill;lemma=Bill;upos=PROPN;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;Word index=5;text=scientist;lemma=scientist;u...</td>\n",
       "      <td>cop</td>\n",
       "      <td>&lt;Word index=2;text=is;lemma=be;upos=AUX;xpos=V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;Word index=5;text=scientist;lemma=scientist;u...</td>\n",
       "      <td>advmod</td>\n",
       "      <td>&lt;Word index=3;text=not;lemma=not;upos=PART;xpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;Word index=5;text=scientist;lemma=scientist;u...</td>\n",
       "      <td>det</td>\n",
       "      <td>&lt;Word index=4;text=a;lemma=a;upos=DET;xpos=DT;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;Word index=0;text=ROOT&gt;</td>\n",
       "      <td>root</td>\n",
       "      <td>&lt;Word index=5;text=scientist;lemma=scientist;u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 gov     rel  \\\n",
       "0  <Word index=5;text=scientist;lemma=scientist;u...   nsubj   \n",
       "1  <Word index=5;text=scientist;lemma=scientist;u...     cop   \n",
       "2  <Word index=5;text=scientist;lemma=scientist;u...  advmod   \n",
       "3  <Word index=5;text=scientist;lemma=scientist;u...     det   \n",
       "4                           <Word index=0;text=ROOT>    root   \n",
       "\n",
       "                                                 dep  \n",
       "0  <Word index=1;text=Bill;lemma=Bill;upos=PROPN;...  \n",
       "1  <Word index=2;text=is;lemma=be;upos=AUX;xpos=V...  \n",
       "2  <Word index=3;text=not;lemma=not;upos=PART;xpo...  \n",
       "3  <Word index=4;text=a;lemma=a;upos=DET;xpos=DT;...  \n",
       "4  <Word index=5;text=scientist;lemma=scientist;u...  "
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "nlp = stanfordnlp.Pipeline()\n",
    "#nlp = stanfordnlp.Pipeline() # This sets up a default neural pipeline in English\n",
    "\n",
    "#doc.sentences.dependencies\n",
    "text = 'Bill is not a scientist'\n",
    "doc = nlp(text)\n",
    "df = pd.DataFrame(doc.sentences[0].dependencies, columns=['gov', 'rel', 'dep'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['staff', 'manager', 'owner']"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nounList=['staff']\n",
    "nounList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(0,len(df)):\n",
    "    if df['rel'][d]=='conj' and df['gov'][d].lemma=='staff':\n",
    "        nounList.append(df['dep'][d].lemma)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<Word index=3;text=staff;lemma=staff;upos=NOUN;xpos=NN;feats=Number=Sing;governor=11;dependency_relation=nsubj>,\n",
       "  'det',\n",
       "  <Word index=1;text=The;lemma=the;upos=DET;xpos=DT;feats=Definite=Def|PronType=Art;governor=3;dependency_relation=det>),\n",
       " (<Word index=3;text=staff;lemma=staff;upos=NOUN;xpos=NN;feats=Number=Sing;governor=11;dependency_relation=nsubj>,\n",
       "  'compound',\n",
       "  <Word index=2;text=hotel;lemma=hotel;upos=NOUN;xpos=NN;feats=Number=Sing;governor=3;dependency_relation=compound>),\n",
       " (<Word index=11;text=friendly;lemma=friendly;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>,\n",
       "  'nsubj',\n",
       "  <Word index=3;text=staff;lemma=staff;upos=NOUN;xpos=NN;feats=Number=Sing;governor=11;dependency_relation=nsubj>),\n",
       " (<Word index=5;text=manager;lemma=manager;upos=NOUN;xpos=NN;feats=Number=Sing;governor=3;dependency_relation=conj>,\n",
       "  'punct',\n",
       "  <Word index=4;text=,;lemma=,;upos=PUNCT;xpos=,;feats=_;governor=5;dependency_relation=punct>),\n",
       " (<Word index=3;text=staff;lemma=staff;upos=NOUN;xpos=NN;feats=Number=Sing;governor=11;dependency_relation=nsubj>,\n",
       "  'conj',\n",
       "  <Word index=5;text=manager;lemma=manager;upos=NOUN;xpos=NN;feats=Number=Sing;governor=3;dependency_relation=conj>),\n",
       " (<Word index=7;text=owner;lemma=owner;upos=NOUN;xpos=NN;feats=Number=Sing;governor=3;dependency_relation=conj>,\n",
       "  'cc',\n",
       "  <Word index=6;text=and;lemma=and;upos=CCONJ;xpos=CC;feats=_;governor=7;dependency_relation=cc>),\n",
       " (<Word index=3;text=staff;lemma=staff;upos=NOUN;xpos=NN;feats=Number=Sing;governor=11;dependency_relation=nsubj>,\n",
       "  'conj',\n",
       "  <Word index=7;text=owner;lemma=owner;upos=NOUN;xpos=NN;feats=Number=Sing;governor=3;dependency_relation=conj>),\n",
       " (<Word index=11;text=friendly;lemma=friendly;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>,\n",
       "  'cop',\n",
       "  <Word index=8;text=were;lemma=be;upos=AUX;xpos=VBD;feats=Mood=Ind|Tense=Past|VerbForm=Fin;governor=11;dependency_relation=cop>),\n",
       " (<Word index=11;text=friendly;lemma=friendly;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>,\n",
       "  'advmod',\n",
       "  <Word index=9;text=not;lemma=not;upos=PART;xpos=RB;feats=_;governor=11;dependency_relation=advmod>),\n",
       " (<Word index=11;text=friendly;lemma=friendly;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>,\n",
       "  'advmod',\n",
       "  <Word index=10;text=very;lemma=very;upos=ADV;xpos=RB;feats=_;governor=11;dependency_relation=advmod>),\n",
       " (<Word index=0;text=ROOT>,\n",
       "  'root',\n",
       "  <Word index=11;text=friendly;lemma=friendly;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>),\n",
       " (<Word index=13;text=supportive;lemma=supportive;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=11;dependency_relation=conj>,\n",
       "  'cc',\n",
       "  <Word index=12;text=and;lemma=and;upos=CCONJ;xpos=CC;feats=_;governor=13;dependency_relation=cc>),\n",
       " (<Word index=11;text=friendly;lemma=friendly;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>,\n",
       "  'conj',\n",
       "  <Word index=13;text=supportive;lemma=supportive;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=11;dependency_relation=conj>),\n",
       " (<Word index=11;text=friendly;lemma=friendly;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>,\n",
       "  'punct',\n",
       "  <Word index=14;text=.;lemma=.;upos=PUNCT;xpos=.;feats=_;governor=11;dependency_relation=punct>)]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sentences[0].dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "det\n",
      "compound\n",
      "nsubj\n",
      "punct\n",
      "conj\n",
      "cc\n",
      "conj\n",
      "cop\n",
      "advmod\n",
      "advmod\n",
      "root\n",
      "cc\n",
      "conj\n",
      "punct\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(doc.sentences[0].dependencies)):\n",
    "    print(doc.sentences[0].dependencies[i][2].dependency_relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc.sentences[0].dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gov: NOUN\n",
      "det\n",
      "dep: DET\n",
      "****\n",
      "gov: NOUN\n",
      "compound\n",
      "dep: NOUN\n",
      "****\n",
      "gov: ADJ\n",
      "nsubj\n",
      "dep: NOUN\n",
      "****\n",
      "gov: NOUN\n",
      "punct\n",
      "dep: PUNCT\n",
      "****\n",
      "gov: NOUN\n",
      "conj\n",
      "dep: NOUN\n",
      "****\n",
      "gov: NOUN\n",
      "cc\n",
      "dep: CCONJ\n",
      "****\n",
      "gov: NOUN\n",
      "conj\n",
      "dep: NOUN\n",
      "****\n",
      "gov: ADJ\n",
      "cop\n",
      "dep: AUX\n",
      "****\n",
      "gov: ADJ\n",
      "advmod\n",
      "dep: PART\n",
      "****\n",
      "gov: ADJ\n",
      "advmod\n",
      "dep: ADV\n",
      "****\n",
      "gov: None\n",
      "root\n",
      "dep: ADJ\n",
      "****\n",
      "gov: ADJ\n",
      "cc\n",
      "dep: CCONJ\n",
      "****\n",
      "gov: ADJ\n",
      "conj\n",
      "dep: ADJ\n",
      "****\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(doc.sentences[0].dependencies)-1):\n",
    "    print(\"gov: \"+ str(doc.sentences[0].dependencies[i][0].upos))\n",
    "    print(doc.sentences[0].dependencies[i][1]) \n",
    "    print(\"dep: \"+ doc.sentences[0].dependencies[i][2].upos)\n",
    "    print(\"****\")               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hotel\n",
      "det\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "print(doc.sentences[0].dependencies[0][0].lemma)\n",
    "print(doc.sentences[0].dependencies[0][1])\n",
    "print(doc.sentences[0].dependencies[0][2].lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<nsubj.?>*<VBD.?>*<JJ.?>*<CC>?'>\n"
     ]
    }
   ],
   "source": [
    "patterns= \"\"\"mychunk:{<nsubj.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
    "chunker = RegexpParser(patterns)\n",
    "print(\"After Regex:\",chunker)\n",
    "#output = chunker.parse(tokens_tag)\n",
    "#print(\"After Chunking\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"The hotel staff and owner were not very friendly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amifaraj/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.StanforCoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('friendly', [Tree('staff', ['The', 'hotel', 'and', 'owner']), 'were', 'not', 'very'])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "sdp = StanfordDependencyParser(path_to_jar='stanford/stanford-corenlp-3.9.2.jar',path_to_models_jar='stanford/stanford-corenlp-3.9.2-models.jar')    \n",
    "\n",
    "result = list(sdp.raw_parse(text))  \n",
    "\n",
    "# print the dependency tree\n",
    "dep_tree = [parse.tree() for parse in result][0]\n",
    "dep_tree\n",
    "\n",
    "# visualize raw dependency tree\n",
    "#from IPython.display import display\n",
    "#display(dep_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. extractDependencies(Sentence)\n",
    "2. getNounSiblings(d.dep)\n",
    "3. getAdjectiveSiblings(d.gov)\n",
    "4. isAffirmative(?)\n",
    "5. isAdjective\n",
    "6. isVerb\n",
    "7. isNoun()\n",
    "8. isPronoun()\n",
    "9. getAdjectiveModifiers()\n",
    "10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractDependencies(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    dependency = doc.sentences[0].dependencies\n",
    "    depTable = pd.DataFrame(dependency, columns=['gov', 'rel', 'dep'])\n",
    "    return depTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNounSibling(depTable, nountoken, adjtoken):\n",
    "    ## I add this ##\n",
    "    if isPronoun(nountoken):\n",
    "        for index in range(0,len(depTable)):\n",
    "            if depTable['gov'][index].lemma == adjtoken.lemma and isNoun(depTable['dep'][index]):\n",
    "                noun = depTable['dep'][index].lemma\n",
    "                nounList=[noun]\n",
    "    elif isNoun(nountoken):\n",
    "        noun = nountoken.lemma\n",
    "        nounList=[noun]\n",
    "    for index in range(0,len(depTable)):\n",
    "        if depTable['rel'][index] =='conj' and depTable['gov'][index].lemma == noun:\n",
    "            nounList.append(depTable['dep'][index].lemma)\n",
    "    return nounList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isdependent(depTable, newadj):\n",
    "    for ind in range(0,len(depTable)):\n",
    "        ## if the newadj is not a gov of nsub which is not dependant on adj\n",
    "        if 'nsubj' in depTable['rel'][ind] and depTable['gov'][ind].lemma == newadj:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAdjectiveSibling(depTable, adj, nouns):\n",
    "    adjective = adj.lemma\n",
    "    adjList=[adjective]\n",
    "    for index in range(0,len(depTable)):\n",
    "        if depTable['rel'][index] =='conj' and depTable['gov'][index].lemma == adjective:\n",
    "            newadj = depTable['dep'][index].lemma\n",
    "            if isdependent(depTable, newadj):  ## if both adj describing same aspect\n",
    "                adjList.append(newadj)\n",
    "    return adjList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAdjectiveModifiers(depTable, adj):\n",
    "    #adjective = adj.lemma    \n",
    "    adjmod='_'\n",
    "    for index in range(0,len(depTable)):\n",
    "        if depTable['rel'][index] =='advmod' and depTable['gov'][index].lemma == adj:\n",
    "            adjmod = depTable['dep'][index].lemma\n",
    "    return adjmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNoun(token):\n",
    "    POS = token.xpos\n",
    "    if 'NN' in POS:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isPronoun(token):\n",
    "    POS = token.xpos\n",
    "    if 'PR' in POS:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isAdjective(token):\n",
    "    POS = token.xpos\n",
    "    if 'JJ' in POS:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isVerb(token):\n",
    "    POS = token.xpos\n",
    "    if 'VB' in POS:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isAdverb(token):\n",
    "    POS = token.xpos\n",
    "    if 'RB' in POS:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeNeg(depTable, Col):\n",
    "    for index in range(0,len(depTable)):\n",
    "        if depTable['rel'][index] =='advmod' and 'Neg' in depTable[Col][index].feats:\n",
    "            depTable['rel'][index] ='neg'\n",
    "    return depTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isAffirmative(depTable, adj):\n",
    "    #adjective = adj.lemma\n",
    "    for index in range(0,len(depTable)):\n",
    "        if depTable['rel'][index] =='neg' and depTable['gov'][index].lemma == adj:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotations(nouns, adj, mod, aff):\n",
    "    #a =[]\n",
    "    for n in nouns:\n",
    "        t = tuple((n, adj, mod, aff))\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casensub(df, index):\n",
    "    dep = df['dep'][index]\n",
    "    gov = df['gov'][index]           # getting the dependence of the noun (looking for adj or vb)\n",
    "    nouns = getNounSibling(df, dep, gov)  #getting the nouns that are joined by conjuction\n",
    "    a =[]\n",
    "    if isAdjective(gov):            ## example: the room is big\n",
    "        adjs = getAdjectiveSibling(df, gov, nouns)  #getting the adjective that are joined by conj\n",
    "        for adj in adjs:                     #for all the adj check the modifier and affirmative\n",
    "            mod = getAdjectiveModifiers(df, adj)\n",
    "            aff = isAffirmative(df, adj)\n",
    "            a.append(annotations(nouns, adj, mod, aff))\n",
    "        return a\n",
    "    elif isVerb(gov):         #example:The staff works fast\n",
    "        for ind in range(0, len(df)):\n",
    "            if (df['rel'][ind] == 'xcomp' or df['rel'][ind] == 'advmod') and df['gov'][ind].lemma == gov.lemma:    ##looking for the adjective or adverb\n",
    "                newdep = df['dep'][ind]                ## adj or adv\n",
    "                adjs = getAdjectiveSibling(df, newdep, nouns)   ## list of adj joined by conj\n",
    "                for adj in adjs:                     #for all the adj check the modifier and affirmative\n",
    "                    mod = getAdjectiveModifiers(df, adj)\n",
    "                    aff = isAffirmative(df, adj)\n",
    "                    a.append(annotations(nouns, adj, mod, aff))\n",
    "                return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caseamod(df, index):   # example: The restaurant has good staff\n",
    "    dep = df['dep'][index]\n",
    "    gov = df['gov'][index]\n",
    "    a =[]\n",
    "    if isAdjective(dep) or isAdverb(dep) and (isNoun(gov) or isPronoun(gov)):\n",
    "        nouns = getNounSibling(df, gov, dep)  #gov is a noun and dep is an adj\n",
    "        adjs = getAdjectiveSibling(df, dep, nouns)  # dep is the adj\n",
    "        for adj in adjs:                     #for all the adj check the modifier and affirmative\n",
    "            mod = getAdjectiveModifiers(df, adj)\n",
    "            aff = isAffirmative(df, adj)\n",
    "            a.append(annotations(nouns, adj, mod, aff))\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casecomp1(df, index):\n",
    "    dep = df['dep'][index]  #adj\n",
    "    gov = df['gov'][index]  #verb\n",
    "    a =[]\n",
    "    if (isNoun(dep) or isPronoun(dep) or isVerb(dep)) or isAdjective(dep):\n",
    "        if isVerb(dep):\n",
    "            noun = getsubj(df, dep)\n",
    "        nouns = getNounSibling(df, dep, gov)\n",
    "        adjs = getAdjectiveSibling(df, gov, nouns)\n",
    "        for adj in adjs:                     #for all the adj check the modifier and affirmative\n",
    "            mod = getAdjectiveModifiers(df, adj)\n",
    "            aff = isAffirmative(df, adj)\n",
    "            a.append(annotations(nouns, adj, mod, aff))\n",
    "        #mods = getAdjectiveModifiers(df, gov)\n",
    "        #aff = isAffirmative(df, dep)\n",
    "        #a = annotations(nouns, adjs, mods, aff)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsubj(depTable, verb):\n",
    "    for ind in range(0,len(depTable)):\n",
    "        if df['rel'][ind] == 'nsubj' and df['gov'][ind].lemma == verb.lemma:\n",
    "            #print(df['dep'][index])\n",
    "            return df['dep'][ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casecomp(df, index):\n",
    "    dep = df['dep'][index]  #adj\n",
    "    gov = df['gov'][index]  #verb\n",
    "    a =[]\n",
    "    if isAdjective(dep):\n",
    "        sub = getsubj(df, gov)\n",
    "        nouns = getNounSibling(df, sub, dep)  #(df, n, adj)\n",
    "        adjs = getAdjectiveSibling(df, dep, nouns)\n",
    "        for adj in adjs:                     #for all the adj check the modifier and affirmative\n",
    "            mod = getAdjectiveModifiers(df, adj)\n",
    "            aff = isAffirmative(df, adj)\n",
    "            a.append(annotations(nouns, adj, mod, aff))\n",
    "        #mods = getAdjectiveModifiers(df, gov)\n",
    "        #aff = isAffirmative(df, dep)\n",
    "        #a = annotations(nouns, adjs, mods, aff)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/amifaraj/stanfordnlp_resources/en_gum_models/en_gum_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_gum', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/home/amifaraj/stanfordnlp_resources/en_gum_models/en_gum_tagger.pt', 'pretrain_path': '/home/amifaraj/stanfordnlp_resources/en_gum_models/en_gum.pretrain.pt', 'lang': 'en', 'shorthand': 'en_gum', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/amifaraj/stanfordnlp_resources/en_gum_models/en_gum_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_gum', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/home/amifaraj/stanfordnlp_resources/en_gum_models/en_gum_parser.pt', 'pretrain_path': '/home/amifaraj/stanfordnlp_resources/en_gum_models/en_gum.pretrain.pt', 'lang': 'en', 'shorthand': 'en_gum', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "#stanfordnlp.download('en', version='0.2.0' )   # This downloads the English models for the neural pipeline\n",
    "# IMPORTANT: The above line prompts you before downloading, which doesn't work well in a Jupyter notebook.\n",
    "# To avoid a prompt when using notebooks, instead use: >>> stanfordnlp.download('en', force=True)\n",
    "nlp = stanfordnlp.Pipeline(lang='en', treebank ='en_gum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gov</th>\n",
       "      <th>rel</th>\n",
       "      <th>dep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;Word index=2;text=game;lemma=game;upos=NOUN;x...</td>\n",
       "      <td>det</td>\n",
       "      <td>&lt;Word index=1;text=the;lemma=the;upos=DET;xpos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;Word index=5;text=cool;lemma=cool;upos=ADJ;xp...</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>&lt;Word index=2;text=game;lemma=game;upos=NOUN;x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;Word index=5;text=cool;lemma=cool;upos=ADJ;xp...</td>\n",
       "      <td>cop</td>\n",
       "      <td>&lt;Word index=3;text=is;lemma=be;upos=AUX;xpos=V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;Word index=5;text=cool;lemma=cool;upos=ADJ;xp...</td>\n",
       "      <td>neg</td>\n",
       "      <td>&lt;Word index=4;text=not;lemma=not;upos=PART;xpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;Word index=0;text=ROOT&gt;</td>\n",
       "      <td>root</td>\n",
       "      <td>&lt;Word index=5;text=cool;lemma=cool;upos=ADJ;xp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;Word index=5;text=cool;lemma=cool;upos=ADJ;xp...</td>\n",
       "      <td>punct</td>\n",
       "      <td>&lt;Word index=6;text=.;lemma=.;upos=PUNCT;xpos=....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 gov    rel  \\\n",
       "0  <Word index=2;text=game;lemma=game;upos=NOUN;x...    det   \n",
       "1  <Word index=5;text=cool;lemma=cool;upos=ADJ;xp...  nsubj   \n",
       "2  <Word index=5;text=cool;lemma=cool;upos=ADJ;xp...    cop   \n",
       "3  <Word index=5;text=cool;lemma=cool;upos=ADJ;xp...    neg   \n",
       "4                           <Word index=0;text=ROOT>   root   \n",
       "5  <Word index=5;text=cool;lemma=cool;upos=ADJ;xp...  punct   \n",
       "\n",
       "                                                 dep  \n",
       "0  <Word index=1;text=the;lemma=the;upos=DET;xpos...  \n",
       "1  <Word index=2;text=game;lemma=game;upos=NOUN;x...  \n",
       "2  <Word index=3;text=is;lemma=be;upos=AUX;xpos=V...  \n",
       "3  <Word index=4;text=not;lemma=not;upos=PART;xpo...  \n",
       "4  <Word index=5;text=cool;lemma=cool;upos=ADJ;xp...  \n",
       "5  <Word index=6;text=.;lemma=.;upos=PUNCT;xpos=....  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "text = \"the game is  not cool.\"\n",
    "#text = \"The staff was not good\"\n",
    "df = extractDependencies(text)\n",
    "df = makeNeg(df, 'dep')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('game', 'cool', '_', False)]]\n"
     ]
    }
   ],
   "source": [
    "A = []\n",
    "for index in range(0, len(df)):\n",
    "    if 'nsubj' in df['rel'][index] and isNoun(df['dep'][index]):\n",
    "        annot = casensub(df, index)\n",
    "        if annot is not None and annot not in A:\n",
    "            A.append(annot)\n",
    "    elif 'amod' in df['rel'][index]:\n",
    "        annot = caseamod(df, index)\n",
    "        if annot is not None and annot not in A:\n",
    "            A.append(annot)\n",
    "    elif 'comp' in df['rel'][index] and isVerb(df['gov'][index]):\n",
    "        annot = casecomp(df, index)\n",
    "        if annot is not None and annot not in A:\n",
    "            A.append(annot)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: What \tupos: PRON\txpos: WP\n",
      "word: would \tupos: AUX\txpos: MD\n",
      "word: happen \tupos: VERB\txpos: VB\n",
      "word: if \tupos: SCONJ\txpos: IN\n",
      "word: the \tupos: DET\txpos: DT\n",
      "word: Indian \tupos: ADJ\txpos: JJ\n",
      "word: government \tupos: NOUN\txpos: NN\n",
      "word: stole \tupos: VERB\txpos: VBD\n",
      "word: the \tupos: DET\txpos: DT\n",
      "word: Kohinoor \tupos: PROPN\txpos: NNP\n",
      "word: ( \tupos: PUNCT\txpos: -LRB-\n",
      "word: Koh \tupos: PROPN\txpos: NNP\n",
      "word: - \tupos: PUNCT\txpos: HYPH\n",
      "word: i \tupos: PROPN\txpos: NNP\n",
      "word: - \tupos: PUNCT\txpos: HYPH\n",
      "word: Noor \tupos: PROPN\txpos: NNP\n",
      "word: ) \tupos: PUNCT\txpos: -RRB-\n",
      "word: diamond \tupos: NOUN\txpos: NN\n",
      "word: back \tupos: ADV\txpos: RB\n",
      "word: ? \tupos: PUNCT\txpos: .\n"
     ]
    }
   ],
   "source": [
    "#import stanfordnlp\n",
    "\n",
    "#nlp = stanfordnlp.Pipeline(processors='tokenize,mwt,pos')\n",
    "doc1 = nlp(\"What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?\")\n",
    "doc2 = nlp(\"What is the story of Kohinoor (Koh-i-Noor) Diamond?\")\n",
    "print(*[f'word: {word.text+\" \"}\\tupos: {word.upos}\\txpos: {word.xpos}' for sent in doc1.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: What \tupos: PRON\txpos: WP\n",
      "word: is \tupos: AUX\txpos: VBZ\n",
      "word: the \tupos: DET\txpos: DT\n",
      "word: story \tupos: NOUN\txpos: NN\n",
      "word: of \tupos: ADP\txpos: IN\n",
      "word: Kohinoor \tupos: PROPN\txpos: NNP\n",
      "word: ( \tupos: PUNCT\txpos: -LRB-\n",
      "word: Koh \tupos: PROPN\txpos: NNP\n",
      "word: - \tupos: PUNCT\txpos: HYPH\n",
      "word: i \tupos: PROPN\txpos: NNP\n",
      "word: - \tupos: PUNCT\txpos: HYPH\n",
      "word: Noor \tupos: PROPN\txpos: NNP\n",
      "word: ) \tupos: PUNCT\txpos: -RRB-\n",
      "word: Diamond \tupos: PROPN\txpos: NNP\n",
      "word: ? \tupos: PUNCT\txpos: .\n"
     ]
    }
   ],
   "source": [
    "print(*[f'word: {word.text+\" \"}\\tupos: {word.upos}\\txpos: {word.xpos}' for sent in doc2.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms: ['thinking', 'thought', 'thought_process', 'cerebration', 'intellection', 'mentation', 'think', 'believe', 'consider', 'conceive', 'think', 'opine', 'suppose', 'imagine', 'reckon', 'guess', 'think', 'cogitate', 'cerebrate', 'remember', 'retrieve', 'recall', 'call_back', 'call_up', 'recollect', 'think', 'think', 'think', 'intend', 'mean', 'think', 'think', 'think', 'think', 'think', 'think', 'think', 'intelligent', 'reasoning', 'thinking']\n",
      "Antonyms: ['forget']\n"
     ]
    }
   ],
   "source": [
    "   #Import wordnet from the NLTK\n",
    "syn = list()\n",
    "ant = list()\n",
    "for synset in wordnet.synsets(\"thinking\"):\n",
    "    for lemma in synset.lemmas():\n",
    "        syn.append(lemma.name())    #add the synonyms\n",
    "        if lemma.antonyms():    #When antonyms are available, add them into the list\n",
    "            ant.append(lemma.antonyms()[0].name())\n",
    "print('Synonyms: ' + str(syn))\n",
    "print('Antonyms: ' + str(ant))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### http://ir.ii.uam.es/aspects/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjective polarity modifiers: A list of adverbs with positive/negative weights, which strengthen, soften or invert the polarity of emphasised adjectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adverb</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>above</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absolutely</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abundantly</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acutely</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazingly</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            weight\n",
       "adverb            \n",
       "above            2\n",
       "absolutely       2\n",
       "abundantly       2\n",
       "acutely          2\n",
       "amazingly        2"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_mod = pd.read_csv('resource_paper/adjective-modifiers.csv', names=['adverb','weight'], header=None)\n",
    "adj_mod.set_index('adverb', inplace = True)\n",
    "adj_mod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adj_mod.loc['readable']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect vocabularies: Lists of nouns referring to aspects of items on several domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noun</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aspect</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>atmosphere</th>\n",
       "      <td>ambiance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atmosphere</th>\n",
       "      <td>ambiances</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atmosphere</th>\n",
       "      <td>ambience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atmosphere</th>\n",
       "      <td>ambiences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atmosphere</th>\n",
       "      <td>atmosphere</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  noun\n",
       "aspect                \n",
       "atmosphere    ambiance\n",
       "atmosphere   ambiances\n",
       "atmosphere    ambience\n",
       "atmosphere   ambiences\n",
       "atmosphere  atmosphere"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspect_voc =pd.read_csv('resource_paper/aspects_books.csv', names=['noun'], header=None)\n",
    "#aspect_voc.reset_index(inplace=True)\n",
    "aspect_voc.index.name = 'aspect'\n",
    "#aspect_voc.rename(columns={\"index\":\"aspect\"},inplace=True)\n",
    "aspect_voc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['aspect'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-443-82dbf5c3ceed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maspect_voc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"aspect\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"noun\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'aspect'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3938\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3939\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3940\u001b[0;31m                                            errors=errors)\n\u001b[0m\u001b[1;32m   3941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3942\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3778\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3779\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3780\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3782\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3810\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3811\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   4963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4964\u001b[0m                 raise KeyError(\n\u001b[0;32m-> 4965\u001b[0;31m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[1;32m   4966\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4967\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['aspect'] not found in axis\""
     ]
    }
   ],
   "source": [
    "aspect_voc.groupby([\"aspect\"]).apply(lambda x: x.sort_values([\"noun\"])).drop(columns=['aspect']).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'aspect'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'aspect'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-440-3a4bea72afcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maspect_voc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'aspect'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'aspect'"
     ]
    }
   ],
   "source": [
    "aspect_voc['aspect'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ambiance', 'ambiances', 'ambience', 'ambiences', 'atmosphere',\n",
       "       'atmospheres', 'light', 'lighting', 'lights', 'allies', 'ally',\n",
       "       'cast', 'casts', 'character', 'character development',\n",
       "       'characters', 'dramatis personae', 'enemies', 'enemy', 'heroe',\n",
       "       'heroes', 'protagonist', 'protagonists', 'role', 'roles',\n",
       "       'villain', 'villains', 'coherence', 'coherences', 'consistencies',\n",
       "       'consistency', 'internal coherence', 'internal coherences',\n",
       "       'internal consistencies', 'internal consistency', 'description',\n",
       "       'descriptions', 'portrait', 'portraits', 'portrayal',\n",
       "       'visual description', 'visual descriptions', 'visual-description',\n",
       "       'visual-descriptions', 'closing', 'closure', 'completion',\n",
       "       'conclusion', 'ending', 'finale', 'language', 'languages', 'tone',\n",
       "       'tones', 'words', 'literary style', 'literary styles',\n",
       "       'narrative style', 'narrative styles', 'style', 'styles',\n",
       "       'writing', 'writing style', 'writing styles', 'cadence',\n",
       "       'cadences', 'pacing', 'rhythm', 'rhythmic score',\n",
       "       'rhythmic scores', 'rhythms', 'chart', 'charts', 'figure',\n",
       "       'figures', 'illustration', 'illustrations', 'image', 'images',\n",
       "       'picture', 'pictures', 'budget', 'budgets', 'charge', 'cost',\n",
       "       'costs', 'economies', 'economy', 'expensive', 'fee', 'fees',\n",
       "       'money', 'price', 'priced', 'prices', 'pricing', 'rate', 'rates',\n",
       "       'scene', 'scenes', 'sequence', 'sequences', 'conversation',\n",
       "       'conversations', 'dialog', 'dialogs', 'dialogue', 'dialogues',\n",
       "       'discussion', 'discussions', 'screenplay', 'screenplays',\n",
       "       'screenwriting', 'script', 'scripts', 'scriptwriting', 'begining',\n",
       "       'intro', 'introduction', 'introductions', 'intros', 'preamble',\n",
       "       'preambles', 'presentation', 'presentations', 'start', 'starting',\n",
       "       'starts', 'narration', 'narrations', 'narrative', 'narratives',\n",
       "       'plot', 'plots', 'screen play', 'screen plays', 'screen-play',\n",
       "       'screen-plays', 'stories', 'story', 'story line', 'story lines',\n",
       "       'story telling', 'storyline', 'story-line', 'storylines',\n",
       "       'story-lines', 'storytelling', 'story-telling', 'issue', 'issues',\n",
       "       'matter', 'matters', 'message', 'messages', 'theme', 'themes',\n",
       "       'topic', 'topics', 'author', 'authors', 'writer', 'writers'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspect_voc['noun'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect-level opinion lexicons: Lists of weighted adjectives associated to item aspects extracted from public reviews on several domains.\n",
    "\n",
    "W_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>aspect</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adj</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>1041</th>\n",
       "      <td>theme</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7th</th>\n",
       "      <th>662</th>\n",
       "      <td>price</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">8th</th>\n",
       "      <th>407</th>\n",
       "      <td>language</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>story</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abhorrent</th>\n",
       "      <th>474</th>\n",
       "      <td>literary_style</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aboriginal</th>\n",
       "      <th>511</th>\n",
       "      <td>pacing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>above-mentioned</th>\n",
       "      <th>742</th>\n",
       "      <td>scenes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">absolute</th>\n",
       "      <th>184</th>\n",
       "      <td>coherence</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>price</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abstract</th>\n",
       "      <th>111</th>\n",
       "      <td>characters</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              aspect  polarity\n",
       "adj                                           \n",
       "2               1041           theme        -1\n",
       "7th             662            price         0\n",
       "8th             407         language        -1\n",
       "                975            story        -1\n",
       "abhorrent       474   literary_style        -1\n",
       "aboriginal      511           pacing         0\n",
       "above-mentioned 742           scenes         0\n",
       "absolute        184        coherence         0\n",
       "                611            price         0\n",
       "abstract        111       characters         0"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinion_lexicon = pd.read_csv('resource_paper/lexicon_books.csv', names=['aspect', 'adj','polarity'], header=None)\n",
    "opinion_lexicon.groupby([\"adj\"]).apply(lambda x: x.sort_values([\"polarity\"])).drop(columns=['adj']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspect</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adj</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sympathetic</th>\n",
       "      <td>atmosphere</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>detailed</th>\n",
       "      <td>atmosphere</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cultural</th>\n",
       "      <td>atmosphere</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ruby</th>\n",
       "      <td>atmosphere</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gothic</th>\n",
       "      <td>atmosphere</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subtle</th>\n",
       "      <td>atmosphere</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human</th>\n",
       "      <td>atmosphere</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sycophantic</th>\n",
       "      <td>atmosphere</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analytical</th>\n",
       "      <td>atmosphere</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dependent</th>\n",
       "      <td>atmosphere</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 aspect  polarity\n",
       "adj                              \n",
       "sympathetic  atmosphere         1\n",
       "detailed     atmosphere         1\n",
       "cultural     atmosphere         0\n",
       "ruby         atmosphere         0\n",
       "gothic       atmosphere         0\n",
       "subtle       atmosphere         1\n",
       "human        atmosphere         0\n",
       "sycophantic  atmosphere         1\n",
       "analytical   atmosphere         0\n",
       "dependent    atmosphere         0"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinion_lexicon.set_index('adj', inplace = True)\n",
    "opinion_lexicon.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinion_lexicon['aspect'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect annotations from reviews: Lists of item aspect annotations extracted from public reviews on several domains, using the generated aspect vocabularies.\n",
    "* userID, itemID, ,aspect, adjective, is_negated, ,adjective_modifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspect_annot = pd.read_csv('resource_paper/annotations_voc_amazon_Books_5.txt', sep='\\t', names=['itemID','noun','aspect', 'adjective', 'is_negated', 'adjective_modifier'], header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aspect_annot.reset_index(inplace=True)\n",
    "aspect_annot.rename(columns={\"index\":\"userID\"},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun = aspect_annot['noun'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>noun</th>\n",
       "      <th>aspect</th>\n",
       "      <th>adjective</th>\n",
       "      <th>is_negated</th>\n",
       "      <th>adjective_modifier</th>\n",
       "      <th>SO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A2S166WSCFIFP5</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>message</td>\n",
       "      <td>theme</td>\n",
       "      <td>powerful</td>\n",
       "      <td>0</td>\n",
       "      <td>so</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A2XQ5LZHTD4AFT</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>style</td>\n",
       "      <td>literary_style</td>\n",
       "      <td>excellent</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3FI0744PG1WYG</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>issues</td>\n",
       "      <td>theme</td>\n",
       "      <td>real</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1340OFLZBW5NG</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>writing</td>\n",
       "      <td>literary_style</td>\n",
       "      <td>flow</td>\n",
       "      <td>0</td>\n",
       "      <td>just</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A19N3FCQCLJYUA</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>picture</td>\n",
       "      <td>pictures</td>\n",
       "      <td>how</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           userID      itemID     noun          aspect  adjective  is_negated  \\\n",
       "0  A2S166WSCFIFP5  000100039X  message           theme   powerful           0   \n",
       "1  A2XQ5LZHTD4AFT  000100039X    style  literary_style  excellent           0   \n",
       "2  A3FI0744PG1WYG  000100039X   issues           theme       real           0   \n",
       "3  A1340OFLZBW5NG  000100039X  writing  literary_style       flow           0   \n",
       "4  A19N3FCQCLJYUA  000100039X  picture        pictures        how           1   \n",
       "\n",
       "  adjective_modifier SO  \n",
       "0                 so     \n",
       "1                NaN     \n",
       "2                NaN     \n",
       "3               just     \n",
       "4                NaN     "
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspect_annot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspect</th>\n",
       "      <th>adjective</th>\n",
       "      <th>is_negated</th>\n",
       "      <th>adjective_modifier</th>\n",
       "      <th>SO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>theme</td>\n",
       "      <td>powerful</td>\n",
       "      <td>0</td>\n",
       "      <td>so</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>literary_style</td>\n",
       "      <td>excellent</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>theme</td>\n",
       "      <td>real</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>literary_style</td>\n",
       "      <td>flow</td>\n",
       "      <td>0</td>\n",
       "      <td>just</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pictures</td>\n",
       "      <td>how</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           aspect  adjective  is_negated adjective_modifier SO\n",
       "0           theme   powerful           0                 so   \n",
       "1  literary_style  excellent           0                NaN   \n",
       "2           theme       real           0                NaN   \n",
       "3  literary_style       flow           0               just   \n",
       "4        pictures        how           1                NaN   "
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = aspect_annot.drop(columns=['userID','itemID','noun'])\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['adj_polarity']=\"\"\n",
    "new_df['adj_mod']=\"\"\n",
    "new_df['is_affirmative']=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment orientation\n",
    "sentiment_orientation = w_aff  w_mod  p_adj\n",
    "* w_aff = is affirmactive (0 or 1) --> is_negated\n",
    "* w_mod = adverb weight (-1, 0.5, 2) --> adj_mod['weight]\n",
    "* p_adj = adjective polarity (-1, 0, +1) --> opinion_lexicon['polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    " def sent_orient(aspect_annot, opinion_lexicon, adj_mod, index):\n",
    "        w_aff = get_waff(df,index)\n",
    "        w_mod = get_wmod(df,index)\n",
    "        p_adj = get_padj(df,index)\n",
    "        print(p_adj)\n",
    "        #print(w_aff*w_mod*p_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_waff(df, index):\n",
    "    if df['is_negated'][index]==0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wmod(df, adj_mod, index):\n",
    "    adv = df['adjective_modifier'][index]\n",
    "    if pd.isnull(adv):\n",
    "        return 1\n",
    "    else:\n",
    "        try:\n",
    "            weight = adj_mod.loc[adv].weight\n",
    "            return weight\n",
    "        except:\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padj(df, opinion_lexicon, index):\n",
    "    \n",
    "    adj = df['adjective'][index]\n",
    "    try:\n",
    "        polarity_list = opinion_lexicon.loc[adj]['polarity'].tolist()\n",
    "        polairty = polarity_list[0]  \n",
    "        return polarity\n",
    "    except:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = new_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amifaraj/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/amifaraj/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:2961: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/amifaraj/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/amifaraj/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "for index in a.index:\n",
    "    a['adj_polarity'][index] = get_padj(a, opinion_lexicon, index)\n",
    "    a['adj_mod'][index]=get_wmod(a, adj_mod, index)\n",
    "    a['is_affirmative'][index]=get_waff(a,index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amifaraj/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspect</th>\n",
       "      <th>adjective</th>\n",
       "      <th>is_negated</th>\n",
       "      <th>adjective_modifier</th>\n",
       "      <th>SO</th>\n",
       "      <th>adj_polarity</th>\n",
       "      <th>adj_mod</th>\n",
       "      <th>is_affirmative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>theme</td>\n",
       "      <td>powerful</td>\n",
       "      <td>0</td>\n",
       "      <td>so</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>literary_style</td>\n",
       "      <td>excellent</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>theme</td>\n",
       "      <td>real</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>literary_style</td>\n",
       "      <td>flow</td>\n",
       "      <td>0</td>\n",
       "      <td>just</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pictures</td>\n",
       "      <td>how</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           aspect  adjective  is_negated adjective_modifier SO adj_polarity  \\\n",
       "0           theme   powerful           0                 so  0            0   \n",
       "1  literary_style  excellent           0                NaN  0            0   \n",
       "2           theme       real           0                NaN  0            0   \n",
       "3  literary_style       flow           0               just  0            0   \n",
       "4        pictures        how           1                NaN  0            0   \n",
       "\n",
       "  adj_mod is_affirmative  \n",
       "0       2              1  \n",
       "1       1              1  \n",
       "2       1              1  \n",
       "3       0              1  \n",
       "4       1              0  "
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['SO'] = a['adj_polarity']*a['adj_mod']*a['is_affirmative']\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "powerful\n",
      "0\n",
      "excellent\n",
      "0\n",
      "real\n",
      "[0, 0]\n",
      "0\n",
      "flow\n",
      "0\n",
      "how\n",
      "0\n",
      "poetic\n",
      "0\n",
      "full\n",
      "0\n",
      "short\n",
      "-1\n",
      "0\n",
      "economic\n",
      "0\n",
      "familiar\n",
      "[1, 1]\n",
      "0\n",
      "beautiful\n",
      "0\n",
      "special\n",
      "1\n",
      "0\n",
      "same\n",
      "0\n",
      "ideological\n",
      "0\n",
      "short\n",
      "-1\n",
      "0\n",
      "literary\n",
      "0\n",
      "0\n",
      "good\n",
      "0\n",
      "speaking\n",
      "0\n",
      "simple\n",
      "[-1, -1, -1, -1]\n",
      "0\n",
      "early\n",
      "0\n",
      "first\n",
      "0\n",
      "arabic\n",
      "0\n",
      "own\n",
      "0\n",
      "beautiful\n",
      "0\n",
      "inspiring\n",
      "0\n",
      "mystical\n",
      "0\n",
      "sheer\n",
      "[0, 0]\n",
      "0\n",
      "poetic\n",
      "0\n",
      "beautiful\n",
      "0\n",
      "ridiculous\n",
      "0\n",
      "deep\n",
      "1\n",
      "0\n",
      "universal\n",
      "1\n",
      "0\n",
      "great\n",
      "0\n",
      "several\n",
      "0\n",
      "wonderful\n",
      "0\n",
      "poetic\n",
      "0\n",
      "brilliant\n",
      "0\n",
      "phenomenal\n",
      "0\n",
      "wise\n",
      "0\n",
      "poetic\n",
      "0\n",
      "spiritual\n",
      "0\n",
      "last\n",
      "0\n",
      "various\n",
      "0\n",
      "important\n",
      "0\n",
      "different\n",
      "0\n",
      "significant\n",
      "0\n",
      "many\n",
      "0\n",
      "basic\n",
      "[-1, -1, -1, -1]\n",
      "0\n",
      "cherished\n",
      "0\n",
      "incredible\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "aspect_annot[\"SO\"] = \"\"\n",
    "for index in aspect_annot.index:\n",
    "    aspect_annot[\"SO\"][index] = sent_orient(aspect_annot, opinion_lexicon, adj_mod, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11702463, 8)"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspect_annot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amifaraj/Documents/Projects/fall-19/WE_reserach/sentiment.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  aspect_annot['SO']=''\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "make_merged_df() takes 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-435-8065b408f81f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentiment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msentiment_orientation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mso\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment_orientation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mso\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment_calc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopinion_lexicon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_mod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/fall-19/WE_reserach/sentiment.py\u001b[0m in \u001b[0;36msentiment_calc\u001b[0;34m(self, aspect_annot, opinion_lexicon, adj_mod)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentiment_calc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect_annot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopinion_lexicon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_mod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0maspect_annot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SO'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mresulted_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_merged_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect_annot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopinion_lexicon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_mod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mresulted_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SO'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresulted_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'adj_polarity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresulted_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'adj_mod'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresulted_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_affirmative'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mresulted_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sent_orientation.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: make_merged_df() takes 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "from sentiment import sentiment_orientation\n",
    "so = sentiment_orientation()\n",
    "result = so.sentiment_calc(a, opinion_lexicon, adj_mod)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Trailing data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-439-b9d07f62daf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'resource_paper/reviews_Video_Games_5.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    535\u001b[0m             )\n\u001b[1;32m    536\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'frame'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'series'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m--> 871\u001b[0;31m                 loads(json, precise_float=self.precise_float), dtype=None)\n\u001b[0m\u001b[1;32m    872\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             decoded = {str(k): v for k, v in compat.iteritems(\n",
      "\u001b[0;31mValueError\u001b[0m: Trailing data"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "df = pd.read_json('resource_paper/reviews_Video_Games_5.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
